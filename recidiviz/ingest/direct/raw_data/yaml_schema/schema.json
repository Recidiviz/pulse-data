{
  "$schema": "http://json-schema.org/draft-07/schema",
  "$id": "https://recidiviz.org/raw_data/yaml_schema/schema.json",
  "title": "Recidiviz Raw Data Manifest",
  "description": "Raw data configuration schema.",
  "type": "object",
  "additionalProperties": false,
  "properties": {
    "file_tag": {
      "description": "The file tag / table name that this file will get written to",
      "type": "string"
    },
    "file_description": {
      "description": "Description of the raw data file contents",
      "type": "string"
    },
    "data_classification": {
      "description": "The file tag / table name that this file will get written to",
      "type": "string",
      "enum": ["source", "validation"]
    },
    "primary_key_cols": {
      "description": "A list of columns that constitute the primary key for this file. If empty, this table cannot be used in an ingest view query and a '*_latest' view will not be generated for this table. May be left empty for the purposes of allowing us to quickly upload a new file into BQ and then determine the primary keys by querying BQ.",
      "type": "array",
      "items": {
        "type": "string"
      },
      "uniqueItems": true
    },
    "is_primary_person_table": {
      "description": "True if this is the overall table representing person (JII) information for this region. All other raw data tables containing person-level information should be able to be joined back to this table, either directly or indirectly, via PERSON_EXTERNAL_ID type columns. Each region may only have one raw file marked as is_primary_person_table; this designation may be arbitrary if there are multiple primary tables representing person information, such as if there are multiple source data systems.",
      "type": "boolean"
    },
    "columns": {
      "description": "A list of names and descriptions for each column in a file.",
      "type": "array",
      "items": {
        "type": "object",
        "required": ["name"],
        "properties": {
          "name": {
            "description": "The column name in BigQuery-compatible, normalized form (e.g. punctuation stripped)",
            "type": "string"
          },
          "description": {
            "description": "Describes the column contents - if None, this column cannot be used for ingest, nor will you be able to write a raw data migration involving this column",
            "type": "string"
          },
          "field_type": {
            "description": "Designates the type of data that this column contains",
            "type": "string",
            "enum": ["string", "datetime","person_external_id", "staff_external_id", "integer"]
          },
          "external_id_type": {
            "description": "If this column holds an external id field_type, designates which type it is (out of the external types defined in common/constants/state/external_id_types.py).",
            "type": "string"
          },
          "is_primary_for_external_id_type": {
            "description": "True if this column holds external ID information in a primary person table. Should only be set for a column of type PERSON_EXTERNAL_ID or STAFF_EXTERNAL_ID. If true for a column of type PERSON_EXTERNAL_ID, then the table that this column belongs to is an ID type root for that region (Note this does not necessarily mean the table is the is_primary_person_table for the region, since there may be multiple tables that are ID type roots for the region.)",
            "type": "boolean"
          },
          "known_values": {
            "description": "Describes possible enum values for this column if known",
            "type": "array",
            "items": {
              "type": "object",
              "required": ["value"],
              "properties": {
                "value": {
                  "description": "The literal enum value",
                  "type": ["string", "integer", "boolean", "null"]
                },
                "description": {
                  "description": "The description that value maps to",
                  "type": ["string", "null"]
                }
              }
            }
          },
          "is_datetime": {
            "description": "True if a column is a date/time",
            "type": "boolean"
          },
          "is_pii": {
            "description": "True if a column contains Personal Identifiable Information (PII)",
            "type": "boolean"
          },
          "datetime_sql_parsers": {
            "description": "Describes the SQL parsers needed to parse the datetime string appropriately. It should contain the string literal {col_name} and follow the format with the SAFE.PARSE_TIMESTAMP('[insert your time format st]', [some expression w/ {col_name}]). SAFE.PARSE_DATE or SAFE.PARSE_DATETIME can also be used. See recidiviz.ingest.direct.views.direct_ingest_big_query_view_types.DATETIME_COL_NORMALIZATION_TEMPLATE",
            "type": "array",
            "items": { "type": "string" }
          },
          "import_blocking_column_validation_exemptions": {
            "description": "A list of dictionaries representing import-blocking validation types that should be exempted for this column.",
            "type": "array",
            "items": {
              "type": "object",
              "required": ["validation_type", "exemption_reason"],
              "properties": {
                "validation_type": {
                  "description": "Import-blocking validation type to exempt",
                  "type": "string",
                  "enum": ["NONNULL_VALUES", "DATETIME_PARSERS", "KNOWN_VALUES", "EXPECTED_TYPE"]
                },
                "exemption_reason": {
                  "description": "The reason for the exemption",
                  "type": "string"
                }
              }
            }
          },
          "null_values": {
            "description": "A list of strings that should be treated as NULL values for this column.",
            "type": "array",
            "items": { "type": "string" }
          }
        }
      },
      "uniqueItems": true
    },
    "table_relationships": {
      "description": "A list of other raw data tables to which this table may be joined, along with valid join logic.",
      "type": "array",
      "items": {
        "type": "object",
        "required": ["foreign_table", "join_logic"],
        "properties": {
          "foreign_table": {
            "description": "The name of the related table in this state's raw data",
            "type": "string"
          },
          "cardinality": {
            "description": "Cardinality of the join. If unspecified, many-to-many is assumed.",
            "type": "string"
          },
          "join_logic": {
            "description": "A list of boolean clauses required for the join. These clauses will be joined together with AND to produce the full join logic.",
            "type": "array",
            "items": {
              "type": "string",
              "pattern": "([^=]+)=([^=]+)"
            }
          },
          "transforms": {
            "description": "A list of dictionaries representing transforms/pre-processing that should be done on columns within this join relationship.",
            "type": "array",
            "items": {
              "type": "object",
              "required": ["column", "transform"],
              "properties": {
                "column": {
                  "description": "The column under transform, formatted as table_name.column_name.",
                  "type": "string"
                },
                "transform": {
                  "description": "The transform to apply, a string including {col_name}.",
                  "type": "string"
                }
              }
            }
          }
        }
      }
    },
    "supplemental_order_by_clause": {
      "description": "An additional string clause that will be added to the ORDER BY list that determines which is the most up-to-date row to pick among all rows that have the same primary key. NOTE: Right now this clause does not have access to the date-normalized version of the columns in datetime_cols,  so must handle its own date parsing logic - if this becomes too cumbersome, we can restructure the query to do  date normalization in a subquery before ordering.",
      "type": "string"
    },
    "encoding": {
      "description": "Most likely string encoding for this file (e.g. UTF-8)",
      "type": "string"
    },
    "separator": {
      "description": "The separator character used to denote columns (e.g. ',' or '|').",
      "type": "string"
    },
    "ignore_quotes": {
      "description": "If true, quoted strings are ignored and separators inside of quotes are treated as column separators. This should be used on any file that has free text fields where the quotes are not escaped and the separator is not common to free text. For example, to handle this row from a pipe separated file that has an open quotation with no close quote: `123|456789|2|He said, \"I will be there.|ASDF`",
      "type": "boolean"
    },
    "custom_line_terminator": {
      "description": "The line terminator character(s) used to denote CSV rows. If None, will default to the Pandas default (any combination of \n and \r).",
      "type": "string"
    },
    "always_historical_export": {
      "description": "If true, means that we **always** will get a historical version of this raw data file from the state and will never change to incremental uploads (for example, because we need to detect row deletions).",
      "type": "boolean"
    },
    "no_valid_primary_keys": {
      "description": "If true, means that the primary keys for this raw data file are unstable over time",
      "type": "boolean"
    },
    "import_chunk_size_rows": {
      "description": "Defines the number of rows in each chunk we will read one at a time from the original raw data file and write back to GCS files before loading into BQ. Increasing this value may increase import speed, but should only be done carefully - if the table has too much data in a row, increasing the number of rows per chunk may push us over VM memory limits. Defaults to 250,000 rows per chunk.",
      "type": "integer"
    },
    "infer_columns_from_config": {
      "description": "If true, means that we likely will receive a CSV that does not have a header row and therefore, we will use the columns defined in the config, in the order they are defined in, as the column names. By default, False.",
      "type": "boolean"
    },
    "update_cadence": {
      "description": "The cadence at which we expect to receive this raw data file (e.g. WEEKLY, DAILY, IRREGULAR)",
      "type": "string"
    },
    "is_code_file": {
      "description": "If true, means that this file is a code table (likely does not contain person-level data and may have few day-over-day updates).",
      "type": "boolean"
    },
    "is_chunked_file": {
      "description": "If true, means that this file tag is received in multiple files (chunks) within a given data transfer.",
      "type": "boolean"
    },
    "expected_number_of_chunks": {
      "description": "The number of distinct files (chunks) we expect to compose this file tag in each file transer",
      "type": "integer"
    },
    "import_blocking_validation_exemptions": {
      "description": "A list of dictionaries representing import-blocking validation types that should be exempted for this table.",
      "type": "array",
      "items": {
        "type": "object",
        "required": ["validation_type", "exemption_reason"],
        "properties": {
          "validation_type": {
            "description": "Import-blocking validation type to exempt",
            "type": "string",
            "enum": ["NONNULL_VALUES", "DATETIME_PARSERS", "KNOWN_VALUES", "EXPECTED_TYPE", "STABLE_HISTORICAL_RAW_DATA_COUNTS"]
          },
          "exemption_reason": {
            "description": "The reason for the exemption",
            "type": "string"
          }
        }
      }
    }
  },
  "required": [
    "file_tag",
    "file_description",
    "data_classification",
    "primary_key_cols",
    "columns"
  ],
  "examples": [
    {
      "file_tag": "data_table",
      "file_description": "This file contains data from `data` system about `table`.",
      "data_classification": "source",
      "primary_key_cols": ["ID"],
      "columns": [
        {
          "name": "ID",
          "description": "Primary key for table",
          "is_pii": true
        },
        {
          "name": "Description",
          "description": "Long description for record"
        },
        {
          "name": "Type",
          "description": "Type of record",
          "known_values": [
            {
              "value": "ADD",
              "description": "This record represents an addition"
            },
            {
              "value": "DEL",
              "description": "This record represents a deletion"
            }
          ]
        }
      ]
    }
  ]
}
