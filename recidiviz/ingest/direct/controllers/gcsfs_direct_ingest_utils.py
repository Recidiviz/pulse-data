# Recidiviz - a data platform for criminal justice reform
# Copyright (C) 2019 Recidiviz, Inc.
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.
# =============================================================================
"""Util functions and types used by GCSFileSystem Direct Ingest code."""

import datetime
import os
import re
from enum import Enum
from typing import Optional

import attr

from recidiviz.cloud_functions.direct_ingest_bucket_name_utils import (
    INGEST_PRIMARY_BUCKET_SUFFIX,
    INGEST_SECONDARY_BUCKET_SUFFIX,
    INGEST_SFTP_BUCKET_SUFFIX,
    build_ingest_bucket_name,
    build_ingest_storage_bucket_name,
    get_region_code_from_direct_ingest_bucket,
)
from recidiviz.cloud_storage.gcsfs_path import (
    GcsfsBucketPath,
    GcsfsDirectoryPath,
    GcsfsFilePath,
)
from recidiviz.common.date import snake_case_datetime
from recidiviz.common.ingest_metadata import SystemLevel
from recidiviz.ingest.direct.errors import DirectIngestError, DirectIngestErrorType
from recidiviz.ingest.direct.types.direct_ingest_instance import DirectIngestInstance
from recidiviz.ingest.direct.types.direct_ingest_instance_factory import (
    DirectIngestInstanceFactory,
)
from recidiviz.ingest.direct.types.direct_ingest_types import CloudTaskArgs
from recidiviz.utils import metadata

INGEST_FILE_PREFIX_REGEX_PATTERN = (
    r"(?P<processed_state>unprocessed|processed)_"  # processed_state
    r"(?P<timestamp>\d{4}-\d{2}-\d{2}T\d{2}[:_]\d{2}[:_]\d{2}[:_]\d{6})_"  # timestamp
    r"((?P<file_type>raw|ingest_view)_)"  # file_type
)

INGEST_FILE_SUFFIX_REGEX_PATTERN = (
    r"(?P<filename_suffix_conflict>-\(\d+\))?"  # Optional file conflict suffix (e.g. '-(1)')
    r"\.(?P<extension>[A-Za-z]+)$"  # Extension
)

INGEST_FILE_TYPE_REGEX = re.compile(INGEST_FILE_PREFIX_REGEX_PATTERN)

RAW_DATA_FILE_NAME_REGEX = re.compile(
    INGEST_FILE_PREFIX_REGEX_PATTERN
    + r"(?P<file_tag>[A-Za-z][A-Za-z\d]*(_[A-Za-z\d]*)*)"  # file_tag
    + INGEST_FILE_SUFFIX_REGEX_PATTERN
)

INGEST_VIEW_FILE_NAME_REGEX = re.compile(
    INGEST_FILE_PREFIX_REGEX_PATTERN
    + r"(?P<file_tag>[A-Za-z][A-Za-z\d]*(_[A-Za-z][A-Za-z\d]*)*)"  # file_tag
    r"(_(?P<filename_suffix>\d+([^-]*)))?"  # Optional filename_suffix
    + INGEST_FILE_SUFFIX_REGEX_PATTERN
)

_FILENAME_SUFFIX_REGEX = re.compile(
    r".*(_file_split(_size(?P<file_split_size_str>\d+))?)$"
)


class GcsfsDirectIngestFileType(Enum):
    """Denotes the type of a file encountered by the BaseDirectIngestController. Files will have their type added to
    the normalized name and this type will be used to determine how to handle the file (import to BigQuery vs ingest
    directly to Postgres). When moved to storage, files with different file types will live in different subdirectories
    in a region's storage bucket."""

    # Raw data received directly from state
    RAW_DATA = "raw"

    # Ingest-ready file
    INGEST_VIEW = "ingest_view"

    @classmethod
    def from_string(cls, type_str: str) -> "GcsfsDirectIngestFileType":
        if type_str == GcsfsDirectIngestFileType.RAW_DATA.value:
            return GcsfsDirectIngestFileType.RAW_DATA
        if type_str == GcsfsDirectIngestFileType.INGEST_VIEW.value:
            return GcsfsDirectIngestFileType.INGEST_VIEW

        raise ValueError(f"Unknown direct ingest file type string: [{type_str}]")


@attr.s(frozen=True)
class GcsfsFilenameParts:
    """A convenience struct that contains information about a file parsed from
    a filename that has been generated by
    cloud_function_utils.py::to_normalized_unprocessed_file_path().

    E.g. Consider the following file path
    "/processed_2019-08-14T23:09:27:047747_elite_offenders_019_historical.csv"

    This will be parsed by filename_parts_from_path() to:
    utc_upload_datetime=datetime.fromisoformat(2019-08-14T23:09:27:047747)
    date_str="2019-08-14"
    file_tag="elite_offenders"
    filename_suffix="019_historical"
    extension="csv"
    """

    processed_state: str = attr.ib()
    utc_upload_datetime: datetime.datetime = attr.ib()
    date_str: str = attr.ib()
    file_type: GcsfsDirectIngestFileType = attr.ib()
    # May contain letters, numbers, and the '_' char. If it contains numbers trailing an _, it must be a RAW_DATA file type.
    file_tag: str = attr.ib()
    # Must start a number and be separated from the file_tag by a '_' char.
    filename_suffix: Optional[str] = attr.ib()
    extension: str = attr.ib()
    is_file_split: bool = attr.ib()
    file_split_size: Optional[int] = attr.ib()

    # File tag followed by file suffix, if there is one
    stripped_file_name: str = attr.ib()

    @stripped_file_name.default
    def _stripped_file_name(self) -> str:
        suffix_str = f"_{self.filename_suffix}" if self.filename_suffix else ""
        return f"{self.file_tag}{suffix_str}"


@attr.s(frozen=True)
class GcsfsIngestArgs(CloudTaskArgs):
    ingest_time: datetime.datetime = attr.ib()
    file_path: GcsfsFilePath = attr.ib()

    def task_id_tag(self) -> str:
        parts = filename_parts_from_path(self.file_path)
        return f"ingest_job_{parts.stripped_file_name}_{parts.date_str}"

    def ingest_instance(self) -> DirectIngestInstance:
        return DirectIngestInstanceFactory.for_ingest_bucket(self.file_path.bucket_path)

    def job_tag(self) -> str:
        """Returns a (short) string tag to identify an ingest run in logs."""
        region_code = (
            get_region_code_from_direct_ingest_bucket(
                self.file_path.bucket_path.bucket_name
            )
            or "unknown_region"
        )
        return (
            f"{region_code.lower()}/{self.file_path.file_name}:" f"{self.ingest_time}"
        )

    @property
    def file_tag(self) -> str:
        return filename_parts_from_path(self.file_path).file_tag


@attr.s(frozen=True)
class GcsfsRawDataBQImportArgs(CloudTaskArgs):
    raw_data_file_path: GcsfsFilePath = attr.ib()

    def task_id_tag(self) -> str:
        parts = filename_parts_from_path(self.raw_data_file_path)
        return f"raw_data_import_{parts.stripped_file_name}_{parts.date_str}"

    def ingest_instance(self) -> DirectIngestInstance:
        return DirectIngestInstanceFactory.for_ingest_bucket(
            self.raw_data_file_path.bucket_path
        )


@attr.s(frozen=True)
class GcsfsIngestViewExportArgs(CloudTaskArgs):
    # The file tag of the ingest view to export. Used to determine which query to run
    # to generate the exported file.
    ingest_view_name: str = attr.ib()

    # The bucket to output the generated ingest view to.
    output_bucket_name: str = attr.ib()

    # The lower bound date for updates this query should include. Any rows that have not
    # changed since this date will not be included.
    upper_bound_datetime_prev: Optional[datetime.datetime] = attr.ib()

    # The upper bound date for updates this query should include. Updates will only
    # reflect data received up until this date.
    upper_bound_datetime_to_export: datetime.datetime = attr.ib()

    def task_id_tag(self) -> str:
        tag = f"ingest_view_export_{self.ingest_view_name}-{self.output_bucket_name}"
        if self.upper_bound_datetime_prev:
            tag += f"-{snake_case_datetime(self.upper_bound_datetime_prev)}"
        else:
            tag += "-None"
        tag += f"-{snake_case_datetime(self.upper_bound_datetime_to_export)}"
        return tag

    def ingest_instance(self) -> DirectIngestInstance:
        return DirectIngestInstanceFactory.for_ingest_bucket(
            GcsfsBucketPath(self.output_bucket_name)
        )


def gcsfs_direct_ingest_temporary_output_directory_path(
    project_id: Optional[str] = None,
) -> GcsfsDirectoryPath:
    if project_id is None:
        project_id = metadata.project_id()
        if not project_id:
            raise ValueError("Project id not set")

    return GcsfsDirectoryPath.from_absolute_path(
        f"{project_id}-direct-ingest-temporary-files"
    )


def bucket_suffix_for_ingest_instance(ingest_instance: DirectIngestInstance) -> str:
    if ingest_instance == DirectIngestInstance.PRIMARY:
        return INGEST_PRIMARY_BUCKET_SUFFIX
    if ingest_instance == DirectIngestInstance.SECONDARY:
        return INGEST_SECONDARY_BUCKET_SUFFIX
    raise ValueError(f"Unexpected ingest instance [{ingest_instance}]")


def gcsfs_direct_ingest_storage_directory_path_for_region(
    *,
    region_code: str,
    system_level: SystemLevel,
    ingest_instance: DirectIngestInstance,
    file_type: Optional[GcsfsDirectIngestFileType] = None,
    project_id: Optional[str] = None,
) -> GcsfsDirectoryPath:
    if project_id is None:
        project_id = metadata.project_id()
        if not project_id:
            raise ValueError("Project id not set")

    suffix = bucket_suffix_for_ingest_instance(ingest_instance)
    bucket_name = build_ingest_storage_bucket_name(
        project_id=project_id,
        system_level_str=system_level.value.lower(),
        suffix=suffix,
    )
    storage_bucket = GcsfsBucketPath(bucket_name)

    if file_type is not None:
        subdir = os.path.join(region_code.lower(), file_type.value)
    else:
        subdir = region_code.lower()
    return GcsfsDirectoryPath.from_dir_and_subdir(storage_bucket, subdir)


def gcsfs_direct_ingest_bucket_for_region(
    *,
    region_code: str,
    system_level: SystemLevel,
    ingest_instance: DirectIngestInstance,
    project_id: Optional[str] = None,
) -> GcsfsBucketPath:
    if project_id is None:
        project_id = metadata.project_id()
        if not project_id:
            raise ValueError("Project id not set")

    suffix = bucket_suffix_for_ingest_instance(ingest_instance)
    bucket_name = build_ingest_bucket_name(
        project_id=project_id,
        region_code=region_code,
        system_level_str=system_level.value.lower(),
        suffix=suffix,
    )
    return GcsfsBucketPath(bucket_name=bucket_name)


def gcsfs_sftp_download_bucket_path_for_region(
    region_code: str, system_level: SystemLevel, project_id: Optional[str] = None
) -> GcsfsBucketPath:
    """Returns the GCS Directory Path for the bucket that will hold the SFTP downloaded files."""
    if project_id is None:
        project_id = metadata.project_id()
        if not project_id:
            raise ValueError("Project id not set")

    bucket_name = build_ingest_bucket_name(
        project_id=project_id,
        region_code=region_code,
        system_level_str=system_level.value.lower(),
        suffix=INGEST_SFTP_BUCKET_SUFFIX,
    )
    return GcsfsBucketPath(bucket_name)


def filename_parts_from_raw_data_path(file_path: GcsfsFilePath) -> GcsfsFilenameParts:
    """Parses filename for RAW_DATA file types"""
    match = re.match(RAW_DATA_FILE_NAME_REGEX, file_path.file_name)

    if not match:
        raise DirectIngestError(
            msg=f"Could not parse upload_ts, file_tag, extension "
            f"from path [{file_path.abs_path()}]",
            error_type=DirectIngestErrorType.INPUT_ERROR,
        )

    full_upload_timestamp_str = match.group("timestamp")
    utc_upload_datetime = datetime.datetime.fromisoformat(full_upload_timestamp_str)
    file_type = GcsfsDirectIngestFileType.from_string(match.group("file_type"))

    return GcsfsFilenameParts(
        processed_state=match.group("processed_state"),
        utc_upload_datetime=utc_upload_datetime,
        date_str=utc_upload_datetime.date().isoformat(),
        file_type=file_type,
        file_tag=match.group("file_tag"),
        extension=match.group("extension"),
        is_file_split=False,
        file_split_size=None,
        filename_suffix=None,
    )


def filename_parts_from_ingest_view_path(
    file_path: GcsfsFilePath,
) -> GcsfsFilenameParts:
    """Parses filename for INGEST_VIEW file types"""
    match = re.match(INGEST_VIEW_FILE_NAME_REGEX, file_path.file_name)

    if not match:
        raise DirectIngestError(
            msg=f"Could not parse upload_ts, file_tag, extension "
            f"from path [{file_path.abs_path()}]",
            error_type=DirectIngestErrorType.INPUT_ERROR,
        )

    full_upload_timestamp_str = match.group("timestamp")
    utc_upload_datetime = datetime.datetime.fromisoformat(full_upload_timestamp_str)
    file_type = GcsfsDirectIngestFileType.from_string(match.group("file_type"))

    filename_suffix = match.group("filename_suffix")
    is_file_split = False
    file_split_size = None

    if filename_suffix:
        filename_suffix_file_split_match = re.match(
            _FILENAME_SUFFIX_REGEX, filename_suffix
        )
        if filename_suffix_file_split_match is not None:
            is_file_split = True
            file_split_size_str = filename_suffix_file_split_match.group(
                "file_split_size_str"
            )
            file_split_size = int(file_split_size_str) if file_split_size_str else None

    return GcsfsFilenameParts(
        processed_state=match.group("processed_state"),
        utc_upload_datetime=utc_upload_datetime,
        date_str=utc_upload_datetime.date().isoformat(),
        file_type=file_type,
        file_tag=match.group("file_tag"),
        extension=match.group("extension"),
        is_file_split=is_file_split,
        file_split_size=file_split_size,
        filename_suffix=filename_suffix,
    )


def filename_parts_from_path(file_path: GcsfsFilePath) -> GcsfsFilenameParts:
    match = re.match(INGEST_FILE_TYPE_REGEX, file_path.file_name)

    if not match:
        raise DirectIngestError(
            msg=f"Could not parse upload_ts, file_tag, extension "
            f"from path [{file_path.abs_path()}]",
            error_type=DirectIngestErrorType.INPUT_ERROR,
        )

    file_type = GcsfsDirectIngestFileType.from_string(match.group("file_type"))

    if file_type is GcsfsDirectIngestFileType.RAW_DATA:
        return filename_parts_from_raw_data_path(file_path)

    if file_type is GcsfsDirectIngestFileType.INGEST_VIEW:
        return filename_parts_from_ingest_view_path(file_path)

    raise DirectIngestError(
        msg=f"Unknown file_type {file_type}, must be one of: {GcsfsDirectIngestFileType.RAW_DATA} "
        f"or {GcsfsDirectIngestFileType.INGEST_VIEW}",
        error_type=DirectIngestErrorType.INPUT_ERROR,
    )
