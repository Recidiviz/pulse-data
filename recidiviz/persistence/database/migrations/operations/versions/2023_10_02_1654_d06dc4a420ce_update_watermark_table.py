# pylint: skip-file
"""update_watermark_table

Revision ID: d06dc4a420ce
Revises: 0696e582f5aa
Create Date: 2023-10-02 16:54:29.054659

"""
import sqlalchemy as sa
from alembic import op
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision = "d06dc4a420ce"
down_revision = "0696e582f5aa"
branch_labels = None
depends_on = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table(
        "direct_ingest_dataflow_job",
        sa.Column("job_id", sa.String(length=255), nullable=False),
        sa.Column("region_code", sa.String(length=255), nullable=False),
        sa.Column(
            "ingest_instance",
            postgresql.ENUM(
                "PRIMARY", "SECONDARY", name="direct_ingest_instance", create_type=False
            ),
            nullable=False,
        ),
        sa.Column("completion_time", sa.DateTime(), nullable=False),
        sa.Column("is_invalidated", sa.Boolean(), nullable=False),
        sa.PrimaryKeyConstraint("job_id"),
    )
    op.create_index(
        op.f("ix_direct_ingest_dataflow_job_ingest_instance"),
        "direct_ingest_dataflow_job",
        ["ingest_instance"],
        unique=False,
    )
    op.create_index(
        op.f("ix_direct_ingest_dataflow_job_region_code"),
        "direct_ingest_dataflow_job",
        ["region_code"],
        unique=False,
    )
    op.add_column(
        "direct_ingest_dataflow_raw_table_upper_bounds",
        sa.Column("watermark_id", sa.INTEGER(), autoincrement=True, nullable=False),
    )
    op.drop_column("direct_ingest_dataflow_raw_table_upper_bounds", "ingest_run_id")
    op.create_primary_key(
        "direct_ingest_dataflow_raw_table_upper_bounds_pkey",
        "direct_ingest_dataflow_raw_table_upper_bounds",
        ["watermark_id"],
    )
    op.add_column(
        "direct_ingest_dataflow_raw_table_upper_bounds",
        sa.Column("job_id", sa.String(length=255), nullable=False),
    )
    op.drop_index(
        "ix_direct_ingest_dataflow_raw_table_upper_bounds_ingest_002b",
        table_name="direct_ingest_dataflow_raw_table_upper_bounds",
    )
    op.create_unique_constraint(
        "file_tags_unique_within_pipeline",
        "direct_ingest_dataflow_raw_table_upper_bounds",
        ["job_id", "raw_data_file_tag"],
    )
    op.create_index(
        op.f("ix_direct_ingest_dataflow_raw_table_upper_bounds_job_id"),
        "direct_ingest_dataflow_raw_table_upper_bounds",
        ["job_id"],
        unique=False,
    )
    op.create_foreign_key(
        "direct_ingest_dataflow_raw_table_upper_bounds_job_id_fkey",
        "direct_ingest_dataflow_raw_table_upper_bounds",
        "direct_ingest_dataflow_job",
        ["job_id"],
        ["job_id"],
        initially="DEFERRED",
        deferrable=True,
    )
    op.drop_column("direct_ingest_dataflow_raw_table_upper_bounds", "is_invalidated")
    op.drop_column("direct_ingest_dataflow_raw_table_upper_bounds", "ingest_instance")
    op.drop_column("direct_ingest_dataflow_raw_table_upper_bounds", "discovery_time")
    op.drop_column("direct_ingest_dataflow_raw_table_upper_bounds", "completion_time")
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column(
        "direct_ingest_dataflow_raw_table_upper_bounds",
        sa.Column(
            "completion_time",
            postgresql.TIMESTAMP(),
            autoincrement=False,
            nullable=False,
        ),
    )
    op.add_column(
        "direct_ingest_dataflow_raw_table_upper_bounds",
        sa.Column("ingest_run_id", sa.INTEGER(), autoincrement=True, nullable=False),
    )
    op.drop_column("direct_ingest_dataflow_raw_table_upper_bounds", "watermark_id")
    op.create_primary_key(
        "direct_ingest_dataflow_raw_table_upper_bounds_pkey",
        "direct_ingest_dataflow_raw_table_upper_bounds",
        ["ingest_run_id"],
    )
    op.add_column(
        "direct_ingest_dataflow_raw_table_upper_bounds",
        sa.Column(
            "discovery_time",
            postgresql.TIMESTAMP(),
            autoincrement=False,
            nullable=False,
        ),
    )
    op.add_column(
        "direct_ingest_dataflow_raw_table_upper_bounds",
        sa.Column(
            "ingest_instance",
            postgresql.ENUM(
                "PRIMARY", "SECONDARY", name="direct_ingest_instance", create_type=False
            ),
            autoincrement=False,
            nullable=False,
        ),
    )
    op.add_column(
        "direct_ingest_dataflow_raw_table_upper_bounds",
        sa.Column("is_invalidated", sa.BOOLEAN(), autoincrement=False, nullable=False),
    )
    op.drop_constraint(
        "direct_ingest_dataflow_raw_table_upper_bounds_job_id_fkey",
        "direct_ingest_dataflow_raw_table_upper_bounds",
        type_="foreignkey",
    )
    op.drop_index(
        op.f("ix_direct_ingest_dataflow_raw_table_upper_bounds_job_id"),
        table_name="direct_ingest_dataflow_raw_table_upper_bounds",
    )
    op.drop_constraint(
        "file_tags_unique_within_pipeline",
        "direct_ingest_dataflow_raw_table_upper_bounds",
        type_="unique",
    )
    op.create_index(
        "ix_direct_ingest_dataflow_raw_table_upper_bounds_ingest_002b",
        "direct_ingest_dataflow_raw_table_upper_bounds",
        ["ingest_instance"],
        unique=False,
    )
    op.drop_column("direct_ingest_dataflow_raw_table_upper_bounds", "job_id")
    op.drop_index(
        op.f("ix_direct_ingest_dataflow_job_region_code"),
        table_name="direct_ingest_dataflow_job",
    )
    op.drop_index(
        op.f("ix_direct_ingest_dataflow_job_ingest_instance"),
        table_name="direct_ingest_dataflow_job",
    )
    op.drop_table("direct_ingest_dataflow_job")
    # ### end Alembic commands ###
